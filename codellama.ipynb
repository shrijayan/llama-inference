{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful coding assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/shrijayan.rajendran/Projects/OS_models/TheBloke:Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  7023.90 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.23 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     3.13 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# model_path = expanduser(\"/Users/shrijayan.rajendran/Projects/OS_models/TheBloke:CodeLlama-13B-GGUF/codellama-13b.Q4_0.gguf\")\n",
    "model_path = expanduser(\"/Users/shrijayan.rajendran/Projects/OS_models/TheBloke:Llama-2-13B-chat-GGUF/llama-2-13b-chat.Q4_0.gguf\")\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    streaming=False,\n",
    "    verbose=True,\n",
    "    n_ctx=2048\n",
    ")\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     647.75 ms\n",
      "llama_print_timings:      sample time =       9.94 ms /    95 runs   (    0.10 ms per token,  9553.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16701.03 ms /   260 tokens (   64.23 ms per token,    15.57 tokens per second)\n",
      "llama_print_timings:        eval time =    7292.12 ms /    94 runs   (   77.58 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:       total time =   24166.60 ms /   354 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! I'll remember the code you provided, so feel free to ask any questions you have about it. What would you like to know? Do you want to know how to use the `add()` method, or perhaps how to implement the other methods (subtraction, multiplication, etc.)? Or maybe you have a specific question about how the `print()` method works? Just let me know and I'll do my best to help!\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain.run(\n",
    "        text='''Remember the code which is shared below so that I will be able to ask questions later.\n",
    "        class Complex {\n",
    "public:\n",
    "  double real;\n",
    "  double imag;\n",
    "\n",
    "  Complex(double r = 0, double i = 0) : real(r), imag(i) {}\n",
    "\n",
    "  Complex add(const Complex& other) {\n",
    "    return Complex(real + other.real, imag + other.imag);\n",
    "  }\n",
    "\n",
    "  // ... Implement other methods for subtraction, multiplication, etc.\n",
    "\n",
    "  void print() {\n",
    "    cout << real << (imag >= 0 ? \"+\" : \"\") << imag << \"i\" << endl;\n",
    "  }\n",
    "};\n",
    "\n",
    "int main() {\n",
    "  Complex c1(2, 3);\n",
    "  Complex c2(1, -5);\n",
    "\n",
    "  Complex sum = c1.add(c2);\n",
    "  sum.print(); // Output: 3+2i\n",
    "\n",
    "  // ... Use other methods of the Complex class\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "'''\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     647.75 ms\n",
      "llama_print_timings:      sample time =      27.47 ms /   256 runs   (    0.11 ms per token,  9318.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7313.87 ms /   112 tokens (   65.30 ms per token,    15.31 tokens per second)\n",
      "llama_print_timings:        eval time =   20909.07 ms /   255 runs   (   82.00 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:       total time =   28636.12 ms /   367 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Certainly! Here's the same code, written in Python:\n",
      "```\n",
      "class Complex:\n",
      "    def __init__(self, real=0, imag=0):\n",
      "        self.real = real\n",
      "        self.imag = imag\n",
      "\n",
      "    def __add__(self, other):\n",
      "        return Complex(self.real + other.real, self.imag + other.imag)\n",
      "\n",
      "    def __sub__(self, other):\n",
      "        return Complex(self.real - other.real, self.imag - other.imag)\n",
      "\n",
      "    def __mul__(self, other):\n",
      "        return Complex(self.real * other.real - self.imag * other.imag,\n",
      "                        2 * self.real * other.imag + self.imag * other.real)\n",
      "\n",
      "    def print(self):\n",
      "        print(f\"{self.real} + {self.imag}i\")\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        return self.real == other.real and self.imag == other.imag\n",
      "\n",
      "c1 = Complex(2, 3)\n",
      "c2 = Complex(1\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(text=\"Can you change the code to python?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     647.75 ms\n",
      "llama_print_timings:      sample time =      27.32 ms /   256 runs   (    0.11 ms per token,  9371.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18396.67 ms /   276 tokens (   66.65 ms per token,    15.00 tokens per second)\n",
      "llama_print_timings:        eval time =   21676.83 ms /   255 runs   (   85.01 ms per token,    11.76 tokens per second)\n",
      "llama_print_timings:       total time =   40513.93 ms /   531 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's the updated Python code with an `average` method added to the `Complex` class:\n",
      "```\n",
      "class Complex:\n",
      "    def __init__(self, real=0, imag=0):\n",
      "        self.real = real\n",
      "        self.imag = imag\n",
      "\n",
      "    def __add__(self, other):\n",
      "        return Complex(self.real + other.real, self.imag + other.imag)\n",
      "\n",
      "    def __sub__(self, other):\n",
      "        return Complex(self.real - other.real, self.imag - other.imag)\n",
      "\n",
      "    def __mul__(self, other):\n",
      "        return Complex(self.real * other.real - self.imag * other.imag,\n",
      "                        2 * self.real * other.imag + self.imag * other.real)\n",
      "\n",
      "    def print(self):\n",
      "        print(f\"{self.real} + {self.imag}i\")\n",
      "\n",
      "    def average(self, other):\n",
      "        return Complex(self.real + other.real, self.imag + other.imag) / 2\n",
      "\n",
      "c1\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(text=\"Can you add a average method to the class?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     647.75 ms\n",
      "llama_print_timings:      sample time =      27.87 ms /   256 runs   (    0.11 ms per token,  9184.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18788.26 ms /   275 tokens (   68.32 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:        eval time =   22094.63 ms /   255 runs   (   86.65 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =   41336.09 ms /   530 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Here's a list of the functions that are defined in the Python code for the `Complex` class:\n",
      "\n",
      "1. `__init__()` - Initializes a new instance of the `Complex` class, taking two arguments for the real and imaginary parts.\n",
      "2. `__add__()` - Implements addition of two complex numbers, returning a new complex number representing the sum of the two inputs.\n",
      "3. `__sub__()` - Implements subtraction of two complex numbers, returning a new complex number representing the difference of the two inputs.\n",
      "4. `__mul__()` - Implements multiplication of two complex numbers, returning a new complex number representing the product of the two inputs.\n",
      "5. `print()` - Prints the complex number in a human-readable format.\n",
      "6. `average()` - Returns the average of two complex numbers, calculated by taking the sum of the real and imaginary parts and dividing by 2.\n",
      "\n",
      "These functions provide the basic arithmetic operations for complex numbers, as well as a way to print the complex numbers in a human-readable format. The `average()` function allows you to calculate the average of two complex numbers, which can be useful in certain\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(text=\"What are the function so the code has?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     647.75 ms\n",
      "llama_print_timings:      sample time =      26.92 ms /   256 runs   (    0.11 ms per token,  9510.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19611.43 ms /   275 tokens (   71.31 ms per token,    14.02 tokens per second)\n",
      "llama_print_timings:        eval time =   22356.07 ms /   255 runs   (   87.67 ms per token,    11.41 tokens per second)\n",
      "llama_print_timings:       total time =   42416.11 ms /   530 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I'd be happy to explain the code! Here's a line-by-line breakdown of what's happening in the Python code for the `Complex` class:\n",
      "\n",
      "1. `class Complex:`: This line defines a new class called `Complex`.\n",
      "2. `def __init__(self, real=0, imag=0):`: This line defines an initializer method for the `Complex` class. The `__init__()` method is called when an instance of the class is created, and it takes two arguments: `real` and `imag`. These arguments are used to initialize the `real` and `imag` attributes of the instance. If no arguments are passed, the default values of `0` are used.\n",
      "3. `self.real = real`: This line sets the `real` attribute of the instance to the value of `real`.\n",
      "4. `self.imag = imag`: This line sets the `imag` attribute of the instance to the value of `imag`.\n",
      "5. `def __add__(self, other):`: This line defines an method for adding two complex numbers. The `__add__()` method takes another `Complex\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(text=\"can you please exaplain the code?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
